# app/orchestrator.py

import asyncio
from datetime import datetime, timezone
from pathlib import Path
import yaml
from app.config import settings
from app.database import PostRepository
from app.models.content import Post
from app.publishers.telegram import TelegramPublisher
from app.scrapers import BaseScraper, ScraperFactory
from app.utils.logger import logger


class SourceConfig:
    def __init__(self, type: str, url: str, enabled: bool = True):
        self.type = type
        self.url = url
        self.enabled = enabled

    def __repr__(self) -> str:
        status = "‚úì" if self.enabled else "‚úó"
        return f"{status} {self.type}: {self.url}"


class Orchestrator:
    def __init__(self):
        self.repository = PostRepository(settings.database_url)
        self.publisher = TelegramPublisher()
        self.scrapers: list[BaseScraper] = []
        self.running = False
        self.start_time = datetime.now(timezone.utc)
        self.cycle_errors = 0
        self.max_consecutive_errors = 5

    async def initialize(self) -> None:
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...")
        settings.ensure_directories()

        try:
            await self.repository.init_db()
            await self.publisher.initialize()
            await self._load_sources()

            if not self.scrapers:
                logger.error("‚ùå –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤!")
                raise RuntimeError("No active sources configured")

            logger.info(f"‚úÖ –ì–æ—Ç–æ–≤. –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(self.scrapers)}")
            logger.info(
                f"‚è∞ –†–µ–∂–∏–º: –ø–æ—Å—Ç—ã –Ω–æ–≤–µ–µ {self.start_time.strftime('%H:%M:%S UTC')}"
            )

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            raise

    async def _load_sources(self) -> None:
        config_path = settings.sources_config

        if not config_path.exists():
            logger.warning(f"‚ö†Ô∏è –ö–æ–Ω—Ñ–∏–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
            self._create_example_config(config_path)
            return

        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            sources_data = config.get("sources", [])
            enabled_count = 0

            for source_data in sources_data:
                source = SourceConfig(
                    type=source_data.get("type", ""),
                    url=source_data.get("url", ""),
                    enabled=source_data.get("enabled", True),
                )

                if not source.enabled:
                    continue

                try:
                    scraper = ScraperFactory.create_scraper(source.type, source.url)
                    self.scrapers.append(scraper)
                    enabled_count += 1
                    logger.info(f"   {source}")
                except ValueError as e:
                    logger.error(f"‚ùå {source.url}: {e}")

            if enabled_count > 0:
                logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {enabled_count}")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞: {e}")

    def _create_example_config(self, path: Path) -> None:
        example_config = {
            "sources": [
                {"type": "telegram", "url": "https://t.me/example", "enabled": False},
                {"type": "vk", "url": "https://vk.com/public123", "enabled": False},
            ]
        }
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            yaml.dump(example_config, f, allow_unicode=True, sort_keys=False)
        logger.info(f"üìù –°–æ–∑–¥–∞–Ω –ø—Ä–∏–º–µ—Ä: {path}")

    async def run(self) -> None:
        if not self.scrapers:
            return

        self.running = True
        logger.info("üîÑ –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞...")
        cycle_num = 0

        try:
            while self.running:
                cycle_num += 1
                logger.info(f"\n{'=' * 50}")
                logger.info(f"üîÑ –¶–∏–∫–ª #{cycle_num}")
                logger.info(f"{'=' * 50}")

                try:
                    await self._scrape_and_publish_cycle()
                    self.cycle_errors = 0

                except Exception as e:
                    self.cycle_errors += 1
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ü–∏–∫–ª–∞ #{cycle_num}: {e}")

                    if self.cycle_errors >= self.max_consecutive_errors:
                        logger.error(
                            f"üí• –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –æ—à–∏–±–æ–∫ ({self.max_consecutive_errors}). "
                            "–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–æ—Ç–∞."
                        )
                        break

                    logger.warning(
                        f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {self.cycle_errors}/{self.max_consecutive_errors}. "
                        "–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É..."
                    )

                logger.info(f"‚è∏Ô∏è –û–∂–∏–¥–∞–Ω–∏–µ {settings.scrape_interval_seconds}s...\n")
                await asyncio.sleep(settings.scrape_interval_seconds)

        except KeyboardInterrupt:
            logger.info("\n‚ö†Ô∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ (Ctrl+C)")
        except Exception as e:
            logger.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        finally:
            await self.shutdown()

    async def _scrape_and_publish_cycle(self) -> None:
        logger.info("üì° –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö...")

        scrape_tasks = [
            scraper.scrape(limit=10, since_time=self.start_time)
            for scraper in self.scrapers
        ]
        results = await asyncio.gather(*scrape_tasks, return_exceptions=True)

        all_posts: list[Post] = []
        errors = 0

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"‚ùå –ò—Å—Ç–æ—á–Ω–∏–∫ #{i + 1}: {result}")
                errors += 1
            elif isinstance(result, list):
                all_posts.extend(result)

        if errors > 0:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–æ–∫ –ø—Ä–∏ —Å–±–æ—Ä–µ: {errors}")

        if not all_posts:
            logger.info("‚ÑπÔ∏è –ù–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –Ω–µ—Ç")
            return

        logger.info(f"üìä –°–æ–±—Ä–∞–Ω–æ: {len(all_posts)}")

        new_count = 0
        published_count = 0
        skipped_old = 0
        skipped_dup = 0

        for post in all_posts:
            try:
                if await self.repository.is_post_processed(post.content_hash):
                    skipped_dup += 1
                    continue

                await self.repository.mark_post_processed(post)
                new_count += 1

                post_date = post.created_at
                if post_date.tzinfo is None:
                    post_date = post_date.replace(tzinfo=timezone.utc)

                if post_date < self.start_time:
                    skipped_old += 1
                    continue

                logger.info(f"üÜï {post.url}")
                logger.debug(f"   ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {settings.post_check_delay_seconds}s...")

                await asyncio.sleep(settings.post_check_delay_seconds)

                try:
                    msg_id = await self.publisher.publish_post(post)
                    await self.repository.mark_post_published(post.content_hash, msg_id)
                    published_count += 1
                    logger.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ (msg_id: {msg_id})")

                except Exception as e:
                    await self.repository.mark_post_failed(post.content_hash, str(e))
                    logger.error(f"‚ùå –ü—É–±–ª–∏–∫–∞—Ü–∏—è –ø—Ä–æ–≤–∞–ª–µ–Ω–∞: {e}")

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–∞: {e}")

        logger.info("\nüìà –ò—Ç–æ–≥:")
        logger.info(f"   –ù–æ–≤—ã—Ö: {new_count}")
        logger.info(f"   –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {published_count}")
        logger.info(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ (—Å—Ç–∞—Ä—ã–µ): {skipped_old}")
        logger.info(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏): {skipped_dup}")

    async def shutdown(self) -> None:
        logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ...")

        for scraper in self.scrapers:
            try:
                await scraper.close()
            except Exception as e:
                logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞: {e}")

        try:
            await self.publisher.close()
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –ø—É–±–ª–∏–∫–∞—Ç–æ—Ä–∞: {e}")

        try:
            await self.repository.close()
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –ë–î: {e}")

        logger.info("üëã –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
